{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23d71e0-31ad-454f-ad57-3e35a8311564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenization: \n",
      " ['Hello', '!', 'How', 'are', 'you', '?', 'I', 'hope', 'you', 'are', 'doing', 'well', '.'] \n",
      "\n",
      "Sentence tokenization: \n",
      " ['Hello!', 'How are you?', 'I hope you are doing well.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "block = \"Hello! How are you? I hope you are doing well.\"\n",
    "\n",
    "# Word tokenization\n",
    "print(\"Word tokenization:\", '\\n', word_tokenize(block), '\\n')\n",
    "\n",
    "# Sentence tokenization\n",
    "print(\"Sentence tokenization:\", '\\n', sent_tokenize(block))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47a2498f-3c1d-4b06-a687-6fd3ee150cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "This is the unclean version-: \n",
      " ['Hello', '!', 'How', 'are', 'you', '?', 'I', 'hope', 'you', 'are', 'doing', 'well', '.'] \n",
      "\n",
      "*********************************************************************************************************************\n",
      "'\n",
      "This is the cleaned version-: \n",
      " ['Hello', '!', 'How', '?', 'I', 'hope', 'well', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Anil\n",
      "[nltk_data]     Rathod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "token = nltk.word_tokenize(block)\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "print(\"This is the unclean version-:\",'\\n', token, '\\n')\n",
    "print(\"*********************************************************************************************************************\\n'\")\n",
    "print(\"This is the cleaned version-:\",'\\n', cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd9c0e31-7097-49da-95ce-f6cb5705f7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rain', 'rain', 'rain', 'rain']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = nltk.PorterStemmer()\n",
    "words = ['rain', 'rained', 'raining', 'rains']\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "print(stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4566cbfe-45a3-44c2-93b7-537be102695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Anil\n",
      "[nltk_data]     Rathod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Anil\n",
      "[nltk_data]     Rathod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'How', '?', 'I', 'hope', 'well', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')#data dependencies\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in cleaned_token]\n",
    "print(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a4d3e2-5e34-49ea-9480-c4cc0a2a8fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NN'), ('!', '.'), ('How', 'WRB'), ('?', '.'), ('I', 'PRP'), ('hope', 'VBP'), ('well', 'RB'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Anil Rathod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged = nltk.pos_tag(cleaned_token)\n",
    "print(tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "253a67b0-bb17-4d2b-b253-fbd8998391d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eac0eef-d66e-4c30-96b7-e5736916315d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'on', 'love', 'and', 'weekends.', 'is', 'to', 'shining.', 'sky', 'blue', 'sun', 'eat', 'the', 'I', 'watch', 'pizza', 'movies', 'The'}\n"
     ]
    }
   ],
   "source": [
    "block_1 = \"The sky is blue and the sun is shining.\"\n",
    "block_2 = \"I love to eat pizza and watch movies on weekends.\"\n",
    "\n",
    "# Splitting the blocks into individual words\n",
    "first_block = block_1.split(\" \")\n",
    "second_block = block_2.split(\" \")\n",
    "\n",
    "# Combining the words to remove duplicate words\n",
    "total_words = set(first_block).union(set(second_block))\n",
    "print(total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6fc6ba6-7c5c-4363-9aed-b022c4bf3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDictA = dict.fromkeys(total_words, 0)\n",
    "wordDictB = dict.fromkeys(total_words, 0)\n",
    "for word in first_block:\n",
    "    wordDictA[word]+=1\n",
    "for word in second_block:\n",
    "    wordDictB[word]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5ac3619-7e40-4ba6-baba-3319bf56c1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>on</th>\n",
       "      <th>love</th>\n",
       "      <th>and</th>\n",
       "      <th>weekends.</th>\n",
       "      <th>is</th>\n",
       "      <th>to</th>\n",
       "      <th>shining.</th>\n",
       "      <th>sky</th>\n",
       "      <th>blue</th>\n",
       "      <th>sun</th>\n",
       "      <th>eat</th>\n",
       "      <th>the</th>\n",
       "      <th>I</th>\n",
       "      <th>watch</th>\n",
       "      <th>pizza</th>\n",
       "      <th>movies</th>\n",
       "      <th>The</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   on  love  and  weekends.  is  to  shining.  sky  blue  sun  eat  the  I  \\\n",
       "0   0     0    1          0   2   0         1    1     1    1    0    1  0   \n",
       "1   1     1    1          1   0   1         0    0     0    0    1    0  1   \n",
       "\n",
       "   watch  pizza  movies  The  \n",
       "0      0      0       0    1  \n",
       "1      1      1       1    0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87e328eb-6eeb-4921-9a74-cd26c1f7448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'weekends.', 'shining.', 'sky', 'blue', 'sun', 'eat', 'I', 'watch', 'pizza', 'movies', 'The']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Anil\n",
      "[nltk_data]     Rathod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in wordDictA if not w in stop_words]\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaeff5ff-5171-481c-a993-a435fedb9972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    on  love       and  weekends.        is   to  shining.       sky  \\\n",
      "0  0.0   0.0  0.111111        0.0  0.222222  0.0  0.111111  0.111111   \n",
      "1  0.1   0.1  0.100000        0.1  0.000000  0.1  0.000000  0.000000   \n",
      "\n",
      "       blue       sun  eat       the    I  watch  pizza  movies       The  \n",
      "0  0.111111  0.111111  0.0  0.111111  0.0    0.0    0.0     0.0  0.111111  \n",
      "1  0.000000  0.000000  0.1  0.000000  0.1    0.1    0.1     0.1  0.000000  \n"
     ]
    }
   ],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_block)\n",
    "tfSecond = computeTF(wordDictB, second_block)\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad005e2f-ad64-4bcc-83b8-c7808071f687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   on  love  and  weekends.  is  to  shining.  sky  blue  sun  eat  the  I  \\\n",
      "0   0     0    1          0   2   0         1    1     1    1    0    1  0   \n",
      "1   1     1    1          1   0   1         0    0     0    0    1    0  1   \n",
      "\n",
      "   watch  pizza  movies  The  \n",
      "0      0      0       0    1  \n",
      "1      1      1       1    0  \n"
     ]
    }
   ],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items(): idfDict[word] = math.log10(N /(float(val) + 1))\n",
    "    return(idfDict)\n",
    "\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "idfs1 = pd.DataFrame([wordDictA, wordDictB])\n",
    "print(idfs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fce99ebc-1946-478f-9d3f-2ba403d24da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         on      love       and  weekends.        is        to  shining.  \\\n",
      "0  0.000000  0.000000  0.033448   0.000000  0.066896  0.000000  0.033448   \n",
      "1  0.030103  0.030103  0.030103   0.030103  0.000000  0.030103  0.000000   \n",
      "\n",
      "        sky      blue       sun       eat       the         I     watch  \\\n",
      "0  0.033448  0.033448  0.033448  0.000000  0.033448  0.000000  0.000000   \n",
      "1  0.000000  0.000000  0.000000  0.030103  0.000000  0.030103  0.030103   \n",
      "\n",
      "      pizza    movies       The  \n",
      "0  0.000000  0.000000  0.033448  \n",
      "1  0.030103  0.030103  0.000000  \n"
     ]
    }
   ],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items(): tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "#putting it in a dataframe\n",
    "idf= pd.DataFrame([idfFirst, idfSecond])\n",
    "print(idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d3fe1c8-da61-4c91-a77e-0df651c36333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The WordNet is a part of Python's Natural Language Toolkit. It is a large word database of English Nouns, Adjectives, Adverbs and Verbs. \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5a1fa75-fff2-451a-8199-53e8fb31aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Make sure all words are in lowercase\n",
    "version_1 = \"Developing a competitive culture.\"\n",
    "version_2 = \"Personalized career guidance.\"\n",
    "vectorize= TfidfVectorizer()\n",
    "#fitting the model and passing our sentences right away:\n",
    "response= vectorize.fit_transform([version_1.lower(), version_2.lower()])           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5758e5f-b0ef-4450-9b37-0e8428824873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.5773502691896257\n",
      "  (0, 1)\t0.5773502691896257\n",
      "  (0, 3)\t0.5773502691896257\n",
      "  (1, 4)\t0.5773502691896257\n",
      "  (1, 0)\t0.5773502691896257\n",
      "  (1, 5)\t0.5773502691896257\n"
     ]
    }
   ],
   "source": [
    " print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d12ce-0609-41e6-9873-5efffdaaa148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
