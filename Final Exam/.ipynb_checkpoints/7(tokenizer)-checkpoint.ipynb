{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3609741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is word wise tokenization-: \n",
      " ['Hello', ',', 'I', 'am', 'Akshata', 'Bidwe', 'currently', 'persuing', 'Grad', 'from', 'Dr.D.Y.Patil', 'Institute', 'of', 'Engineering', ',', 'Management', ',', 'and', 'Research', '.', 'I', 'love', 'to', 'travel', '.', 'My', 'birthplace', 'is', 'nashik', '.'] \n",
      "\n",
      "-----------------------------------------------------------------------------'\n",
      "'\n",
      "This is sentence wise tokenization-: \n",
      " ['Hello, I am Akshata Bidwe currently persuing Grad from Dr.D.Y.Patil Institute of Engineering,Management, and Research.', 'I love to travel.', 'My birthplace is nashik.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "block=\"Hello, I am Akshata Bidwe currently persuing Grad from Dr.D.Y.Patil Institute of Engineering,Management, and Research. I love to travel. My birthplace is nashik.\"\n",
    "print(\"This is word wise tokenization-:\",'\\n', nltk.word_tokenize(block), '\\n')\n",
    "print(\"-----------------------------------------------------------------------------'\\n'\")\n",
    "print(\"This is sentence wise tokenization-:\",'\\n', nltk.sent_tokenize(block))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d1dc1d-81a2-477d-8ba6-fa2b52bf858e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "738e62cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "This is the unclean version-: \n",
      " ['Hello', ',', 'I', 'am', 'Akshata', 'Bidwe', 'currently', 'persuing', 'Grad', 'from', 'Dr.D.Y.Patil', 'Institute', 'of', 'Engineering', ',', 'Management', ',', 'and', 'Research', '.', 'I', 'love', 'to', 'travel', '.', 'My', 'birthplace', 'is', 'nashik', '.'] \n",
      "\n",
      "----------------------------------------------------------------------'\n",
      "'\n",
      "This is the cleaned version-: \n",
      " ['Hello', ',', 'I', 'Akshata', 'Bidwe', 'currently', 'persuing', 'Grad', 'Dr.D.Y.Patil', 'Institute', 'Engineering', ',', 'Management', ',', 'Research', '.', 'I', 'love', 'travel', '.', 'My', 'birthplace', 'nashik', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "token = nltk.word_tokenize(block)\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "print(\"This is the unclean version-:\",'\\n', token, '\\n')\n",
    "print(\"----------------------------------------------------------------------'\\n'\")\n",
    "print(\"This is the cleaned version-:\",'\\n', cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b47f9c-381b-4fca-a957-83c612f664ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb267c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['argu', 'argu', 'argu']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = nltk.PorterStemmer()\n",
    "words = ['argued','arguing','argue','causes']\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "print(stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ca788f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'I', 'Akshata', 'Bidwe', 'currently', 'persuing', 'Grad', 'Dr.D.Y.Patil', 'Institute', 'Engineering', ',', 'Management', ',', 'Research', '.', 'I', 'love', 'travel', '.', 'My', 'birthplace', 'nashik', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')#data dependencies\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "words = ['argued','arguing','argue','causes']\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f1e2be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), (',', ','), ('I', 'PRP'), ('Akshata', 'VBP'), ('Bidwe', 'NNP'), ('currently', 'RB'), ('persuing', 'VBG'), ('Grad', 'NNP'), ('Dr.D.Y.Patil', 'NNP'), ('Institute', 'NNP'), ('Engineering', 'NNP'), (',', ','), ('Management', 'NNP'), (',', ','), ('Research', 'NNP'), ('.', '.'), ('I', 'PRP'), ('love', 'VBP'), ('travel', 'NN'), ('.', '.'), ('My', 'PRP$'), ('birthplace', 'NN'), ('nashik', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged = nltk.pos_tag(cleaned_token)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d732f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is', 'mind', '', 'the', 'culture', 'aim', 'among', 'develop', 'good', 'interest', 'a', 'of', 'Keeping', 'Our', 'work', 'to', 'students', 'professionals', 'IT', 'in'}\n"
     ]
    }
   ],
   "source": [
    "block_1 = \"Our aim is to develop a good work culture among students\"\n",
    "block_2 = \"Keeping in mind the interest of the IT professionals \"\n",
    "#split so each word have their own string\n",
    "first_block = block_1.split(\" \")\n",
    "second_block = block_2.split(\" \")\n",
    "#join them to remove common duplicate words\n",
    "total= set(first_block).union(set(second_block))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "490639fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'mind': 0, '': 0, 'the': 0, 'culture': 1, 'aim': 1, 'among': 1, 'develop': 1, 'good': 1, 'interest': 0, 'a': 1, 'of': 0, 'Keeping': 0, 'Our': 1, 'work': 1, 'to': 1, 'students': 1, 'professionals': 0, 'IT': 0, 'in': 0} {'is': 0, 'mind': 1, '': 1, 'the': 2, 'culture': 0, 'aim': 0, 'among': 0, 'develop': 0, 'good': 0, 'interest': 1, 'a': 0, 'of': 1, 'Keeping': 1, 'Our': 0, 'work': 0, 'to': 0, 'students': 0, 'professionals': 1, 'IT': 1, 'in': 1}\n"
     ]
    }
   ],
   "source": [
    "wordDictA = dict.fromkeys(total, 0)\n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_block:\n",
    "    wordDictA[word]+=1\n",
    "for word in second_block:\n",
    "    wordDictB[word]+=1\n",
    "print(wordDictA, wordDictB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aac37436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mind', '', 'culture', 'aim', 'among', 'develop', 'good', 'interest', 'Keeping', 'Our', 'work', 'students', 'professionals', 'IT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in wordDictA if not w in stop_words]\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36d1f6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         is  mind       the   culture       aim     among   develop      good  \\\n",
      "0  0.090909   0.0  0.0  0.0  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
      "1  0.000000   0.1  0.1  0.2  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "   interest         a   of  Keeping       Our      work        to  students  \\\n",
      "0       0.0  0.090909  0.0      0.0  0.090909  0.090909  0.090909  0.090909   \n",
      "1       0.1  0.000000  0.1      0.1  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "   professionals   IT   in  \n",
      "0            0.0  0.0  0.0  \n",
      "1            0.1  0.1  0.1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_block)\n",
    "tfSecond = computeTF(wordDictB, second_block)\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1ff03e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   is  mind  the  culture  aim  among  develop  good\n",
      "0   1     0    0        1    1      1        1     1\n",
      "1   0     1    1        2    0      0        0     0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items(): idfDict[word] = math.log10(N /(float(val) + 1))\n",
    "    return(idfDict)\n",
    "\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "idfs1 = pd.DataFrame([wordDictA, wordDictB])\n",
    "print(idfs1)\n",
    "\n",
    "# Sample word count dictionaries (wordDictA and wordDictB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09d0d7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         is      mind       the   culture       aim     among   develop  \\\n",
      "0  0.023931  0.000000  0.023931  0.023931  0.023931  0.023931  0.023931   \n",
      "1  0.000000  0.030103  0.026324  0.052648  0.000000  0.000000  0.000000   \n",
      "\n",
      "       good  \n",
      "0  0.023931  \n",
      "1  0.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    return {word: val * idfs[word] for word, val in tfBow.items()}\n",
    "\n",
    "# Sample TF dictionaries\n",
    "tfFirst = {'is': 0.090909, 'mind': 0.0, 'the': 0.090909, 'culture': 0.090909, 'aim': 0.090909, 'among': 0.090909, 'develop': 0.090909, 'good': 0.090909}\n",
    "tfSecond = {'is': 0.0, 'mind': 0.1, 'the': 0.1, 'culture': 0.2, 'aim': 0.0, 'among': 0.0, 'develop': 0.0, 'good': 0.0}\n",
    "\n",
    "# IDF dictionary\n",
    "idfs = {word: math.log10(2 / (float(val) + 1)) for word, val in tfFirst.items()}\n",
    "\n",
    "# Compute TF-IDF for the sample TF dictionaries using the IDF dictionary\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "\n",
    "# Create a DataFrame from the TF-IDF dictionaries\n",
    "idf = pd.DataFrame([idfFirst, idfSecond])\n",
    "print(idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab0956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1cc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07984b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf0716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The WordNet is a part of Python's Natural Language Toolkit. It is a large word database of English Nouns, Adjectives, Adverbs and Verbs. \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e727cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Make sure all words are in lowercase\n",
    "version_1 = \"Developing a competitive culture where the students polish technical and professional attributes, gain experience and learn new skills while upgrading the already present skillset. For those fledglings who have a zeal to build a strong profile and are hunting for their Ikigai, CSI provides ample opportunities for those individuals too.\"\n",
    "version_2 = \"Personalized career guidance, Regular Logic and aptitude building activities, Industrial level project collaboration, Building a network with active collaborations across the globe, Periodic member exclusive conferences and seminars, Created a community for sharing skills and knowledge\"\n",
    "#calling the TfidfVectorizer\n",
    "vectorize= TfidfVectorizer()\n",
    "#fitting the model and passing our sentences right away:\n",
    "response= vectorize.fit_transform([version_1.lower(), version_2.lower()])           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5110fb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 61)\t0.13915271943780658\n",
      "  (0, 31)\t0.13915271943780658\n",
      "  (0, 40)\t0.13915271943780658\n",
      "  (0, 4)\t0.13915271943780658\n",
      "  (0, 48)\t0.13915271943780658\n",
      "  (0, 18)\t0.13915271943780658\n",
      "  (0, 30)\t0.13915271943780658\n",
      "  (0, 58)\t0.13915271943780658\n",
      "  (0, 29)\t0.13915271943780658\n",
      "  (0, 7)\t0.13915271943780658\n",
      "  (0, 46)\t0.13915271943780658\n",
      "  (0, 54)\t0.13915271943780658\n",
      "  (0, 9)\t0.13915271943780658\n",
      "  (0, 60)\t0.13915271943780658\n",
      "  (0, 67)\t0.13915271943780658\n",
      "  (0, 28)\t0.13915271943780658\n",
      "  (0, 65)\t0.13915271943780658\n",
      "  (0, 23)\t0.13915271943780658\n",
      "  (0, 59)\t0.27830543887561315\n",
      "  (0, 24)\t0.2970249178760062\n",
      "  (0, 53)\t0.13915271943780658\n",
      "  (0, 44)\t0.13915271943780658\n",
      "  (0, 3)\t0.13915271943780658\n",
      "  (0, 62)\t0.13915271943780658\n",
      "  (0, 64)\t0.13915271943780658\n",
      "  :\t:\n",
      "  (1, 21)\t0.16649349332910351\n",
      "  (1, 37)\t0.16649349332910351\n",
      "  (1, 41)\t0.16649349332910351\n",
      "  (1, 26)\t0.16649349332910351\n",
      "  (1, 0)\t0.16649349332910351\n",
      "  (1, 13)\t0.16649349332910351\n",
      "  (1, 1)\t0.16649349332910351\n",
      "  (1, 66)\t0.16649349332910351\n",
      "  (1, 38)\t0.16649349332910351\n",
      "  (1, 12)\t0.16649349332910351\n",
      "  (1, 47)\t0.16649349332910351\n",
      "  (1, 35)\t0.16649349332910351\n",
      "  (1, 32)\t0.16649349332910351\n",
      "  (1, 2)\t0.16649349332910351\n",
      "  (1, 10)\t0.33298698665820703\n",
      "  (1, 6)\t0.16649349332910351\n",
      "  (1, 36)\t0.16649349332910351\n",
      "  (1, 49)\t0.16649349332910351\n",
      "  (1, 27)\t0.16649349332910351\n",
      "  (1, 11)\t0.16649349332910351\n",
      "  (1, 42)\t0.16649349332910351\n",
      "  (1, 24)\t0.11846149176425531\n",
      "  (1, 52)\t0.11846149176425531\n",
      "  (1, 5)\t0.35538447529276596\n",
      "  (1, 57)\t0.11846149176425531\n"
     ]
    }
   ],
   "source": [
    " print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273cc7e-f7ad-48ba-9411-8d136fe0dd16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
